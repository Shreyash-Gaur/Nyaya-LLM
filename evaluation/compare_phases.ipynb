{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nyaya-LLM ‚Äî Phase 1 vs Phase 2 Comparison\n",
    "\n",
    "Evaluates the best model's **Phase 1 adapter** vs **Phase 2 adapter** on `eval_set.json`.\n",
    "\n",
    "**80 curated questions across 4 categories:**\n",
    "- `Statute Accuracy` ‚Äî factual recall from trained acts\n",
    "- `Hypothetical Scenario` ‚Äî applying law to real situations\n",
    "- `Hallucination Test` ‚Äî traps with fake/repealed sections\n",
    "- `Generalization` ‚Äî legal concepts without section numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install peft bitsandbytes accelerate huggingface_hub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "login(token=user_secrets.get_secret(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Imports done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ‚öôÔ∏è  CONFIG ‚Äî edit these to match your setup\n",
    "# ==========================================\n",
    "\n",
    "# Base model ‚Äî whichever won Phase 1\n",
    "BASE_MODEL = \"microsoft/Phi-4-mini-instruct\"\n",
    "# BASE_MODEL = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "# BASE_MODEL = \"google/gemma-3-4b-it\"\n",
    "\n",
    "# Adapter dataset\n",
    "ADAPTER_DATASET = \"/kaggle/input/datasets/shreyashgaurgla/nyaya-adapters\"\n",
    "\n",
    "# Phase 1 adapter ‚Äî best from Phase 1 eval\n",
    "PHASE_1_ADAPTER = f\"{ADAPTER_DATASET}/qlora_phase1_phi4_mini/qlora_phase1_phi4_mini\"\n",
    "# PHASE_1_ADAPTER = f\"{ADAPTER_DATASET}/lora_phase1_phi4_mini/lora_phase1_phi4_mini\"\n",
    "\n",
    "# Phase 2 adapter ‚Äî same model trained on augmented data\n",
    "PHASE_2_ADAPTER = f\"{ADAPTER_DATASET}/qlora_phase2_phi4_mini/qlora_phase2_phi4_mini\"\n",
    "# PHASE_2_ADAPTER = f\"{ADAPTER_DATASET}/lora_phase2_phi4_mini/lora_phase2_phi4_mini\"\n",
    "\n",
    "# Eval set\n",
    "EVAL_SET_PATH = \"/kaggle/input/datasets/shreyashgaurgla/nyaya-eval/eval_set.json\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_FILE  = \"/kaggle/working/compare_phases_results.json\"\n",
    "RESULTS_DIR  = \"/kaggle/working\"\n",
    "JUDGE_RETRIES = 3\n",
    "\n",
    "print(\"Config loaded.\")\n",
    "print(f\"  Base model  : {BASE_MODEL}\")\n",
    "print(f\"  Phase 1     : {PHASE_1_ADAPTER}\")\n",
    "print(f\"  Phase 2     : {PHASE_2_ADAPTER}\")\n",
    "print(f\"  Eval set    : {EVAL_SET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üí¨  JUDGE PROMPT\n",
    "# Hallucination category: correctly rejecting\n",
    "# a fake section scores 5, fabricating = 1\n",
    "# ==========================================\n",
    "JUDGE_PROMPT_TEMPLATE = \"\"\"You are a strict legal AI evaluator. Score the MODEL ANSWER against the REFERENCE ANSWER.\n",
    "\n",
    "SCORING RUBRIC:\n",
    "  5 - Perfect. Legally accurate, complete, no errors.\n",
    "  4 - Good. Correct core content, minor omissions or slight imprecision.\n",
    "  3 - Acceptable. Gets the general idea but misses important legal details.\n",
    "  2 - Poor. Partially correct but contains notable legal errors.\n",
    "  1 - Wrong. Fabricated law, completely incorrect, or irrelevant answer.\n",
    "\n",
    "IMPORTANT:\n",
    "- Score based on semantic correctness, NOT exact wording match.\n",
    "- The reference may be long. The model only needs to capture the key legal meaning.\n",
    "- If the model answer is legally equivalent to the reference but phrased differently, score it high.\n",
    "- For HALLUCINATION TEST questions: a model that correctly says a section does not exist scores 5.\n",
    "  A model that invents a plausible-sounding answer scores 1.\n",
    "\n",
    "QUESTION:\n",
    "{instruction}\n",
    "\n",
    "REFERENCE ANSWER:\n",
    "{reference}\n",
    "\n",
    "MODEL ANSWER:\n",
    "{prediction}\n",
    "\n",
    "Respond ONLY with a valid JSON object, nothing else:\n",
    "{{\"score\": <int 1-5>, \"reasoning\": \"<one concise sentence>\"}}\"\"\"\n",
    "\n",
    "print(\"Judge prompt ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ü§ñ  GENERATION\n",
    "# ==========================================\n",
    "def generate_response(model, tokenizer, instruction: str) -> str:\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return full_output.split(\"### Response:\\n\")[-1].strip()\n",
    "\n",
    "print(\"generate_response() ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üßë‚Äç‚öñÔ∏è  JUDGE ‚Äî HuggingFace\n",
    "# Same judge as evaluate-phase1.ipynb\n",
    "# ==========================================\n",
    "judge_pipe = None\n",
    "\n",
    "def load_judge():\n",
    "    global judge_pipe\n",
    "    print(\"Loading judge model (Qwen2.5-7B 4-bit)...\")\n",
    "\n",
    "    judge_bnb = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    judge_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        quantization_config=judge_bnb,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    judge_model.generation_config.max_length = None\n",
    "\n",
    "    judge_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "    judge_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=judge_model,\n",
    "        tokenizer=judge_tokenizer,\n",
    "    )\n",
    "    print(\"Judge loaded.\\n\")\n",
    "\n",
    "\n",
    "def judge_score(instruction: str, reference: str, prediction: str) -> tuple:\n",
    "    prompt = JUDGE_PROMPT_TEMPLATE.format(\n",
    "        instruction=instruction,\n",
    "        reference=reference[:600],\n",
    "        prediction=prediction[:600]\n",
    "    )\n",
    "\n",
    "    for attempt in range(JUDGE_RETRIES):\n",
    "        try:\n",
    "            output = judge_pipe(\n",
    "                prompt,\n",
    "                max_new_tokens=150,\n",
    "                min_new_tokens=10,\n",
    "                do_sample=False,\n",
    "                return_full_text=False,\n",
    "                pad_token_id=judge_pipe.tokenizer.eos_token_id\n",
    "            )\n",
    "            response = output[0][\"generated_text\"].strip()\n",
    "            response = re.sub(r\"```(?:json)?\", \"\", response).strip()\n",
    "\n",
    "            if not response:\n",
    "                raise ValueError(\"Empty response from judge\")\n",
    "\n",
    "            match = re.search(r\"\\{.*?\\}\", response, re.DOTALL)\n",
    "            if not match:\n",
    "                raise ValueError(f\"No JSON found. Raw: {response[:150]}\")\n",
    "\n",
    "            parsed = json.loads(match.group())\n",
    "            score  = int(parsed[\"score\"])\n",
    "\n",
    "            if not (1 <= score <= 5):\n",
    "                raise ValueError(f\"Score out of range: {score}\")\n",
    "\n",
    "            return score, parsed.get(\"reasoning\", \"\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è  Judge attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt == JUDGE_RETRIES - 1:\n",
    "                return 0, \"Judge error ‚Äî skipped\"\n",
    "\n",
    "    return 0, \"Judge error ‚Äî skipped\"\n",
    "\n",
    "print(\"Judge functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üìä  SUMMARY PRINTER\n",
    "# ==========================================\n",
    "def print_summary(results: list):\n",
    "    categories = [\n",
    "        \"Statute Accuracy\",\n",
    "        \"Hypothetical Scenario\",\n",
    "        \"Hallucination Test\",\n",
    "        \"Generalization\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä  PHASE 1 vs PHASE 2 ‚Äî FINAL COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    phase_avgs = {}\n",
    "\n",
    "    for phase in [\"Phase_1\", \"Phase_2\"]:\n",
    "        phase_results = [r for r in results if r[\"model\"] == phase]\n",
    "        valid         = [r for r in phase_results if r[\"score\"] > 0]\n",
    "\n",
    "        if not valid:\n",
    "            print(f\"\\n{phase}: No valid scores.\")\n",
    "            continue\n",
    "\n",
    "        overall = sum(r[\"score\"] for r in valid) / len(valid)\n",
    "        phase_avgs[phase] = overall\n",
    "\n",
    "        print(f\"\\n  {phase}:\")\n",
    "        print(f\"    Overall avg : {overall:.2f} / 5.0  (n={len(valid)}/{len(phase_results)})\")\n",
    "        print(f\"    By category :\")\n",
    "\n",
    "        for cat in categories:\n",
    "            cat_scores = [r[\"score\"] for r in valid if r[\"category\"] == cat]\n",
    "            if cat_scores:\n",
    "                avg = sum(cat_scores) / len(cat_scores)\n",
    "                bar = \"‚ñà\" * int(avg)\n",
    "                print(f\"      {cat:<25} {avg:.2f}  {bar}  (n={len(cat_scores)})\")\n",
    "\n",
    "    # Delta table\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"  DELTA (Phase 2 - Phase 1):\")\n",
    "\n",
    "    p1_valid = [r for r in results if r[\"model\"] == \"Phase_1\" and r[\"score\"] > 0]\n",
    "    p2_valid = [r for r in results if r[\"model\"] == \"Phase_2\" and r[\"score\"] > 0]\n",
    "\n",
    "    for cat in categories:\n",
    "        p1_scores = [r[\"score\"] for r in p1_valid if r[\"category\"] == cat]\n",
    "        p2_scores = [r[\"score\"] for r in p2_valid if r[\"category\"] == cat]\n",
    "        if p1_scores and p2_scores:\n",
    "            p1_avg = sum(p1_scores) / len(p1_scores)\n",
    "            p2_avg = sum(p2_scores) / len(p2_scores)\n",
    "            delta  = p2_avg - p1_avg\n",
    "            arrow  = \"‚¨ÜÔ∏è \" if delta > 0.05 else (\"‚¨áÔ∏è \" if delta < -0.05 else \"‚û°Ô∏è \")\n",
    "            print(f\"    {cat:<25} P1={p1_avg:.2f}  P2={p2_avg:.2f}  {arrow} {delta:+.2f}\")\n",
    "\n",
    "    if \"Phase_1\" in phase_avgs and \"Phase_2\" in phase_avgs:\n",
    "        overall_delta = phase_avgs[\"Phase_2\"] - phase_avgs[\"Phase_1\"]\n",
    "        arrow = \"‚¨ÜÔ∏è \" if overall_delta > 0.05 else (\"‚¨áÔ∏è \" if overall_delta < -0.05 else \"‚û°Ô∏è \")\n",
    "        print(f\"\\n    {'OVERALL':<25} P1={phase_avgs['Phase_1']:.2f}  P2={phase_avgs['Phase_2']:.2f}  {arrow} {overall_delta:+.2f}\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(\"print_summary() ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üöÄ  MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "    # Load eval set\n",
    "    print(f\"Loading eval set from: {EVAL_SET_PATH}\")\n",
    "    with open(EVAL_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_data = json.load(f)\n",
    "    print(f\"Loaded {len(eval_data)} questions.\\n\")\n",
    "\n",
    "    # Verify categories\n",
    "    from collections import Counter\n",
    "    cat_counts = Counter(item[\"category\"] for item in eval_data)\n",
    "    print(\"Category breakdown:\")\n",
    "    for cat, count in sorted(cat_counts.items()):\n",
    "        print(f\"  {cat:<25} {count} questions\")\n",
    "    print()\n",
    "\n",
    "    # Load judge once ‚Äî stays loaded for both phases\n",
    "    load_judge()\n",
    "\n",
    "    # Load base model once\n",
    "    print(f\"Loading base model: {BASE_MODEL}...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=(\"qwen\" in BASE_MODEL.lower()),\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        trust_remote_code=(\"qwen\" in BASE_MODEL.lower())\n",
    "    )\n",
    "    print(\"Base model loaded.\\n\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # ‚îÄ‚îÄ Evaluate both phases ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    for phase_name, adapter_path in [\n",
    "        (\"Phase_1\", PHASE_1_ADAPTER),\n",
    "        (\"Phase_2\", PHASE_2_ADAPTER)\n",
    "    ]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîÑ  {phase_name} ‚Äî Loading adapter...\")\n",
    "        print(f\"    {adapter_path}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "            model.eval()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Could not load {phase_name} adapter: {e}\")\n",
    "            continue\n",
    "\n",
    "        phase_written = 0\n",
    "\n",
    "        for i, item in enumerate(tqdm(eval_data, desc=phase_name), 1):\n",
    "            instruction = item[\"prompt\"]\n",
    "            reference   = item[\"reference\"]\n",
    "            category    = item[\"category\"]\n",
    "            item_id     = item.get(\"id\", f\"{i:03d}\")\n",
    "\n",
    "            # Generate answer\n",
    "            answer = generate_response(model, tokenizer, instruction)\n",
    "\n",
    "            # Judge scores it\n",
    "            score, reasoning = judge_score(instruction, reference, answer)\n",
    "\n",
    "            print(f\"  [{i:02d}/{len(eval_data)}] [{category}] Score: {score}/5 ‚Äî {reasoning[:80]}\")\n",
    "\n",
    "            results.append({\n",
    "                \"model\":           phase_name,\n",
    "                \"category\":        category,\n",
    "                \"id\":              item_id,\n",
    "                \"prompt\":          instruction,\n",
    "                \"reference\":       reference,\n",
    "                \"answer\":          answer,\n",
    "                \"score\":           score,\n",
    "                \"judge_reasoning\": reasoning,\n",
    "                \"timestamp\":       datetime.now().isoformat()\n",
    "            })\n",
    "            phase_written += 1\n",
    "\n",
    "        # Save after each phase so you don't lose Phase 1 if Phase 2 crashes\n",
    "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\n‚úÖ {phase_name} done ‚Äî {phase_written} questions scored.\")\n",
    "        print(f\"üíæ Intermediate save ‚Üí {OUTPUT_FILE}\")\n",
    "\n",
    "        # Unload adapter before loading Phase 2\n",
    "        print(f\"Unloading {phase_name} adapter...\")\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Final save\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nüíæ Final results saved ‚Üí {OUTPUT_FILE}\")\n",
    "\n",
    "    # Print comparison\n",
    "    print_summary(results)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
